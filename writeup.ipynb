{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Sentiment analysis of financial news is a powerful tool for understanding market trends and investor sentiment. By analyzing statements from financial experts, it provides insights into the market’s direction without requiring individuals to manually interpret every piece of news. Accurately classifying sentiment in financial statements can help investors, analysts, and decision-makers respond to market conditions more effectively.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The goal of this project is to develop a machine learning model that classifies the sentiment of financial news statements as positive, neutral, or negative. This serves as both a practical application and a learning opportunity to deepen my understanding of natural language processing (NLP) and its applications in finance. Through this project, I explore different text vectorization techniques, model selection, and ensemble learning strategies to improve sentiment classification performance.\n",
    "\n",
    "### High-Level Summary of Results\n",
    "\n",
    "The key takeaways from this project include:\n",
    "\n",
    "- Gaining a deeper understanding of the pros and cons of different text vectorization methods (TF-IDF, Bag of Words, and Word Embeddings).\n",
    "- Building a stack ensemble model to improve sentiment classification performance.\n",
    "- Comparing the ensemble model’s accuracy to individual machine learning models and evaluating statistical significance using McNemar’s test.\n",
    "- Addressing and mitigating data leakage, ensuring a robust and fair model evaluation.\n",
    "\n",
    "### Data Overview\n",
    "\n",
    "The dataset used for training the models is a financial phrase bank, where financial statements were annotated by 16 domain experts. Each phrase was classified into sentiment categories (positive, neutral, or negative) based on the level of agreement among the annotators. The dataset is split into four different files based on the percentage of agreement: 50%, 66%, 75%, and 100%.\n",
    "\n",
    "Initial Approach\n",
    "\n",
    "My initial plan was:\n",
    "\n",
    "- __Training Data__: Combine the 50% and 66% agreement files to maximize the number of training examples, allowing the model to learn from a diverse set of financial statements.\n",
    "- __Tuning Data__: Use the 75% agreement file for hyperparameter tuning.\n",
    "- __Test Data__: Use the 100% agreement file to evaluate final model performance.\n",
    "\n",
    "However, this plan ultimately failed due to data leakage, which I will discuss in detail later. As a result, I had to pivot to a more traditional approach—removing overlapping phrases and manually splitting the data into training, tuning, and testing sets.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "1. Data Preprocessing \n",
    "    \n",
    "    Before combining all of the files together into one dataset I decided to first check and remove the duplicates. I also split the text files along the delimiter '@' which allowed for creation fof a pandas dataframe that contains the phrase in one column and the sentiment in another column. I split the data by using 80% of the data as the training set, 10% as the tuning set, and the final 10% of the dataset as the test set. \n",
    "\n",
    "2. Text Vectorization \n",
    "\n",
    "    Text vectorization is the process of converting words (text data) into numerical representations so that machine learning models can process and analyze them. There are three main methods of text vectorization:\n",
    "\n",
    "    - Bog of Words (BoW)\n",
    "\n",
    "        BoW creates a vocabulary of unique words from the text, counts how often each word appears in each document, and represents each document as a vector of word counts.\n",
    "\n",
    "        Pro's\n",
    "\n",
    "        - Simple and easy to understand.\n",
    "        - Works well for basic text classification tasks\n",
    "        \n",
    "        Con's\n",
    "\n",
    "        - Produces sparse, binary-like vectors that do not capture word meaning or importance.\n",
    "        - Ignores word order and context.\n",
    "    \n",
    "    - Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "        TF-IDF builds upon BoW by weighting words based on their importance. Frequently occurring words (e.g., “the,” “is”) are given lower weights, while rarer words receive higher scores.\n",
    "\n",
    "        Pro's \n",
    "\n",
    "        - Reduces the impact of common words while emphasizing more meaningful ones.\n",
    "        - Useful for feature selection in NLP tasks\n",
    "        \n",
    "        Con's\n",
    "\n",
    "        - Does not capture the important or meaning of the word.\n",
    "        - Weights are purely based on frequency, which may not always be optimal.\n",
    "        \n",
    "    \n",
    "    - Word Embeddings \n",
    "\n",
    "        Word embeddings represent words in high-dimensional spaces where similar words have similar vector representations. Examples include Word2Vec, GloVe, and FastText.\n",
    "    \n",
    "        Pro's \n",
    "\n",
    "        - Preserves the semantic relatinonship between words.\n",
    "        - More effective for capturing meaning and context.\n",
    "        \n",
    "        Con's \n",
    "\n",
    "        - Requires large datsets and significant computation resources for effective training.\n",
    "        - Pretrained embeddings may not always generalize well to specialized domains.\n",
    "\n",
    "    For the purposes of this project, we will be using the Term Frequency - Inverse Document Frequency (TF-IDF) because it captures the importance of words without the requirement of a large dataset.\n",
    "\n",
    "    3. Machine Learning Models\n",
    "\n",
    "        The machine learning models that I used for this project were **Multinomial Naive Bayes**, **Logistic Regression**, **K Nearest Neighbors**, and **Random Forests**. \n",
    "\n",
    "        - Multinomial Naive Bayes (MNB)\n",
    "        \n",
    "            Probabilistic machine learning algorithm that is primarily used for text classification such as spam detection, document classification, and sentiment analysis. A specializaed form of Naive Bayes designed for discrete data, making it extremely well-suited for Natural Language Processing Applications.\n",
    "\n",
    "            - Fast and Efficient: Works well with high-dimensional text data\n",
    "            - Performs well with small datasets\n",
    "            - Handles word frequency well\n",
    "\n",
    "            However, MNB assumes independence between words, something that is not always true in language. \n",
    "        \n",
    "        - Logistic Regression \n",
    "\n",
    "            A supervised learning algorithm used for classification tasks. It is used for predicting categorical outcomes rather than regression. Widely applied in areas such as sentiment analysis, spam detection, and medical diagnosis.\n",
    "\n",
    "            - Interpretable: Coefficients can explain feature importance \n",
    "            - Efficient: Works well on small-to-medium datasets\n",
    "            - Handles high-dimensional data well\n",
    "            - Works with TF-IDF and BoW representations\n",
    "\n",
    "            One of the main drawbacks for logistic regression is that it can only learn linear decision boundaries, it struggles when relationships between the features are highly non-linear.\n",
    "        \n",
    "        - K-Nearest Neighbors (KNN)\n",
    "\n",
    "            A simple, non-parametric machine learning algorithm used for classification and regression. It makes predictions based on the K most similar (nearest) data points in the training set.\n",
    "\n",
    "            - Works well for problems with multiple lables \n",
    "            - Works with different Vectorization techniques \n",
    "\n",
    "            However, one major drawback is that text data or large datasets leads to high-dimensional feature spaces, making distance calculations less meaningful.\n",
    "        \n",
    "        - Random Forests\n",
    "\n",
    "            An ensemble learning method that builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting. \n",
    "\n",
    "            - Handles high-dimensional data well.\n",
    "            - Robust to overfitting.\n",
    "            - Can handle imbalanced data.\n",
    "\n",
    "            One of the main drawbacks of Random Forests in the context of text analysis is that it does not capture word meaning. Another drawback it is computationally expensive compared to simpler machine learning methods like Logistic Resgression and MNB.\n",
    "        \n",
    "        - Stacked Ensemble Learning \n",
    "\n",
    "            An advanced machine learning technique that combines multiple base models to create a meta-model that makes more accurate predictions. Instead of relying on a single algorithm, stackign takes the strength of multiple models and learns how best to weigh their predictions. \n",
    "\n",
    "            - Combines the strength of multiple models, reducing the bias and variance by leveraging different mdoels' unique advantages.\n",
    "            - Handles edge cases better than individual models.\n",
    "            Reduces overfitting by ensuring the final model does not overly rely on one pattern. \n",
    "\n",
    "            Some drawbacks of stack ensemble learning include increased computation complexity, as it requires training multiple models leading to longer processing times, and hyperparameter tuning complexity. \n",
    "    \n",
    "### Model Evaluation Metrics \n",
    "\n",
    "To evaluate the performance of our models, we used precision, recall, and F1-score. With a greater emphasis on overall classification accuracy. Since the dataset has three sentiment classes (positive, neutral, negative), we also examined confusion matrices to understand model misclassifications. \n",
    "\n",
    "Prediction -> Out of all predicted sentiments of one class, how many actually belong to that class?\n",
    "Accuracy -> Out of all actual sentiments of one class, how many were correctly predicted?\n",
    "F1-Score -> Harmonic mean of precision and recall (balances both).\n",
    "\n",
    "| Model | Accuracy (Tune) | Accuracy (Test) | Precision (Neg, Neu, Pos) | Recall Precision (Neg, Neu, Pos) | F1-Score Precision (Neg, Neu, Pos) |\n",
    "| ----- | --------------- | --------------- | --------- | ------ | -------- |\n",
    "| Multinomial NB | 0.7038 | 0.6921 | (0.90, 0.70, 0.69) | (0.15, 0.98, 0.36) | (0.25, 0.81, 0.47) |\n",
    "| Multinomial NB (Tuned) | 0.7391 | 0.7087 | (0.74, 0.75, 0.67) | (0.46, 0.92, 0.47) | (0.57, 0.83, 0.55) |\n",
    "| Logistic Regression | 0.7723 | 0.7376 | (0.87, 0.74, 0.71) | (0.33, 0.93, 0.51) | (0.48, 0.82, 0.60) | \n",
    "| KNN | 0.7226 | 0.6818 | (0.50, 0.75, 0.57) | (0.49, 0.84, 0.43) | (0.50, 0.79, 0.49) |\n",
    "| KNN (Tuned) | 0.7143 | 0.7293 | (0.67, 0.77, 0.66) | (0.43, 0.91, 0.50) | (0.52, 0.83, 0.57) |\n",
    "| Random Forest | 0.7536 | 0.7748 | (0.87, 0.77, 0.77) | (0.44, 0.95, 0.56) | (0.59, 0.85, 0.65) | \n",
    "| Random Forest (Tuned) | 0.7619 | 0.7707 | (0.87, 0.76, 0.76) | (0.44, 0.94, 0.55) | (0.59, 0.84, 0.64) |\n",
    "| Stack Ensemble | 0.7826 | 0.7810 | (0.76, 0.81, 0.74) | (0.52, 0.91, 0.65) | (0.62, 0.86, 0.70) |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Results Interpretation & Discussion\n",
    "\n",
    "The primary objective of this project was to develop a machine learning model capable of classifying financial news sentiments as positive, neutral, negative. The results show that the stack ensemble model achieved the highest accuracy on the test set, closely followed by the general Random Forest model. However, certain trends emerged when evaluating performance across different sentiment categories. \n",
    "\n",
    "- Model Performance Breakdown by Sentiment \n",
    "\n",
    "    Analyzing the classification report, we observe clear disparities in model performance across sentiment categories:\n",
    "\n",
    "    - Negative Sentiment \n",
    "\n",
    "        >All models struggled with recall for negative sentiment (meaning they miss a lot of negative phrases). Negative statements tend to be more nuanced which could mislead the model into thinking that a certain phrases is neutral or positive. \"Despite record revenue, uncertainty looms over regulartory changes.\" The words \"record revenue\" may mislead the model into thinking that this phrase is positive or neutral. The best individual model, general Random Forest, identified 44% of negative cases correctly, while Stack Ensemble improves this to 52%. This increase in recall suggests that combining models like MNB (which handles word frequency well) and RF (which finds patterns in high-dimensional data) might help to mitigate misclassification. \n",
    "    \n",
    "    - Neutral Sentiment\n",
    "\n",
    "        >Every model performed best on neutral sentiment with recall values (~0.90+). Neutral statements often contain more common, factual, and non-emotional words that are easier for models to classify. \"The company announced its quarterly earnings report today.\" Even simpler models like Naive Bayes performed well meaning that the words associated with neutral statements are more distinct and consistent. When comparing Stack Ensemble and RF, we see that they performed almost identically, 0.91 recall for Stack and 0.95 recall for RF. Since positive negative phrases often drive action, correctly classifying neutral phrases is less impactful in sentiment-driven decision-making.\n",
    "\n",
    "    - Positive Sentiment\n",
    "\n",
    "        >Precision was generally lower for positive sentiment than for neutral sentiment but still much better than negative sentiment. The reason for this could be that positive phrases often share characteristics with neutral phrases but could more subtle. \"The company exceeded expectations but remains cautious about future performance.\" The words \" exceeded expectations\" sounds positive, but \"remainds cautious\" introduces uncertainty, making classification harder. Stack Ensemble helps by balancing recall and precision better than individual models, boosting the F1-Score to 0.70 from 0.65 when using the RF model, but precision and recall remain suboptimal, indicating room for improvement. \n",
    "    \n",
    "    To determine whether Stack Ensemble's improvement over Random Forest was statistically significant, we performed McNemar's test, which evaluates whether two models have meaningfully different misclassfication patterns. The resulting test statistics was 2.8 with a p-value of 0.0906. This indicating no statistically significant difference between the stack ensemble model and the general RF model. While the stack ensemble model achieved slightly higher accuracy, it is not strong enough to conclude it is meaningfully better.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Practical Implementations of the Results\n",
    "\n",
    "Since there is no statistically significant difference between the general RF model and the stack ensemble model, it would make sense to stick with the RF model. This is due to RF model's being less computationally expensive, and is comparatively simpler than a stack ensemble model. In a real-world financial application, these models would definitely need to be touched up and improved upon. Introducing deep learning models would also be a way to improve the accuracy of sentiment analysis, however, this would come with its own challenges - a major one being that there would be a need for a much larger and diverse dataset.\n",
    "        \n",
    "### Limitations of the Approach \n",
    "\n",
    "1. Data Limitations\n",
    "\n",
    "    The dataset was relatively small which could mean that there were patterns the machine learning models did not fully intergrate into its system due to a lack of examples. \n",
    "\n",
    "    Since the dataset was annotated by humans, there could be slight bias that was introduced. The files that contained different percentages of agreed upon phrases were combined into one larger dataset. This step introduced more subjectivity into the overall dataset, which could have played a role in the machine learning models getting confused by the lack of agreement on the 50% phrases.\n",
    "\n",
    "2. Feature Representation Limitations\n",
    "\n",
    "    The TF-IDF vectorization method used in this project does not capture word relationships, meaning, or context.\n",
    "\n",
    "    Phrase-based sentiment is oftentimes more complex than word based sentiment as they could contain both positive and negative elements, increasing the difficulty of classification. \n",
    "\n",
    "    Word embedding would have enhanced model performance by capturing semantic meaning of phrases. However, a larger dataset would have been needed in order to properly utilize this. \n",
    "\n",
    "3. Modeling Limitations\n",
    "\n",
    "    Hyperparameter tuning did not significantly improve performance, and even lowered it in the case of RF models. This suggests that either the default hyperparameters were already well-optimized, or the dataset size was too small for tuning to provide meaningful results. \n",
    "\n",
    "    The lack of deep learning models incorporated in this project means that the mdoel may struggle with nuanced sentiments, only being able to rely on traditional machine learning approaches. \n",
    "\n",
    "### Future Work \n",
    "\n",
    "1. Exploring Advanced NLP Models \n",
    "\n",
    "    Implement word embeddings such as Word2Vec, FastText, or GloVe instead of TF-IDF\n",
    "\n",
    "    Experiment with transformer-based models such as BERT, RoBERTa, or FinBERT. These models can understand context better and may perform significantly better for financial sentiment analysis tasks. \n",
    "\n",
    "    Test LSTMs or GRUs, which recurrent neural networks designed to handle sequential text data.\n",
    "\n",
    "2. Annotation Subjectivity and Bias \n",
    "\n",
    "    The dataset was annotated by humans, meaning there is inherent subjectivity in how phrases were classified as positive, neutral, or negative. \n",
    "\n",
    "    Combining the datasets with different levels of annotator agreements (50%, 66%, 75% and 100%) likely introduced some inconsistencies, as mentioned previously, making it harder for models to learn clear distinctions between sentiment categories. \n",
    "\n",
    "    Specifically, phrases in the 50% agreement category may have had more ambiguous sentiment, potentially confusing the models and leading to lower performance on negative sentiment classification. \n",
    "\n",
    "3. Financial Domain-Specific Language \n",
    "\n",
    "    The dataset consists only of short financial phrases, rather than full financial reports or news articles. \n",
    "\n",
    "    Financial text often contains complex language, techincal jargon, and context-dependent phrases, which the models may struggle to to interpret correctly. \n",
    "\n",
    "    Expanding the datasets to include long-form financial news or earnings reports couuld improve model performance in real-world applications. \n",
    "\n",
    "\n",
    "### Conclusion \n",
    "\n",
    "The goal of this project was to teach myself to develop a machine learning model capable of accurately classifying financial news statements into positive, neutral, or negative sentiments. \n",
    "\n",
    "The results showed that while the Stack Ensemble model has the highest accuracy, McNemar's test revealed its improvement over the Random Forest model was not statistically significant. Given the computational efficiency and relative simplicity of Random Forest, it would likely be the preferred model in real-world applications unless further improvements to ensemble techinques demonstrate a meaningful advantage. \n",
    "\n",
    "Additionally, the analysis highlighted challenges in sentiment classification, particularly in distinguishing negative statements, which were frequently misclassified as neutral or positive. The models performed best on neutral statementts, likely due to their consistent linguistic patterns. Future work could focus on handling nuanced language, incorporating deep learning methods, and expanding the dataset to improve model robustness. \n",
    "\n",
    "Overall, this project provided valuable insights into sentiment analysis, natural language processing (NLP), and machine learning techniques, reinforcing the important of data preprocessing, model evaluation, and statistical validation in predictive modeling. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
